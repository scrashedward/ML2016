#! /usr/bin/env python

import pickle
import numpy as np
import keras
import sys
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.optimizers import SGD
from keras import backend as K

def new_model(shape):
        model = Sequential()

        model.add(Convolution2D(32, 3, 3, border_mode='same',
                                dim_ordering = 'tf',
                                input_shape=shape))
        model.add(Activation('relu'))
        model.add(Convolution2D(32, 3, 3))
        model.add(Activation('relu'))
        model.add(MaxPooling2D(pool_size=(2, 2), dim_ordering = 'tf'))
        model.add(Dropout(0.25))

        model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering = 'tf'))
        model.add(Activation('relu'))
        model.add(Convolution2D(64, 3, 3))
        model.add(Dropout(0.25))

        model.add(Flatten())
        model.add(Dense(512))
        model.add(Activation('relu'))
        model.add(Dropout(0.5))
        model.add(Dense(nb_classes))
        model.add(Activation('softmax'))

        model.compile(loss='categorical_crossentropy',
                      optimizer='adam',
                      metrics=['accuracy'])
        return model
def new_datagen():
 # this will do preprocessing and realtime data augmentation
	datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images
	return datagen

batch_size = 100
nb_classes = 10
nb_epoch = 100
data_augmentation = True
SemiSupervised = False
filePath = sys.argv[1]
modelName = sys.argv[2]
if len(sys.argv) == 4:
	SemiSupervised = True

#print K.image_dim_ordering();

# input image dimensions
img_rows, img_cols = 32, 32
img_channels = 3

f = open( filePath + 'all_label.p', 'rb')

all_label = pickle.load(f)
all_label = np.asarray(all_label)
shape = all_label.shape
all_label = all_label.reshape(shape[0]*shape[1], 3, 32, 32)
all_label = all_label.swapaxes(1,2).swapaxes(2,3)

label = np.zeros((500,1))

for i in range(1,10):
	label = np.concatenate((label, np.ones((500, 1)) * i))
	#print label.shape

label = np_utils.to_categorical(label.astype(int), nb_classes)


#f = open( filePath + '/test.p', 'rb')

#test = pickle.load(f)
#test = np.asarray(test['data'])
#shape = test.shape
#test = test.reshape(shape[0], 3, 32, 32)
#test = test.swapaxes(1,2).swapaxes(2,3)

f = open(filePath + 'all_unlabel.p', 'rb')
unlabel = pickle.load(f)
unlabel = np.asarray(unlabel)
shape = unlabel.shape
unlabel = unlabel.reshape(shape[0], 3, 32, 32)
unlabel = unlabel.swapaxes(1,2).swapaxes(2,3)

all_label = all_label.astype('float32')
#test = test.astype('float32')
unlabel = unlabel.astype('float32')
all_label /= 255
#test /= 255
unlabel /= 255

model = new_model(all_label.shape[1:])

if not data_augmentation:
    print('Not using data augmentation.')
    model.fit(all_label, label,
              batch_size=batch_size,
              nb_epoch=nb_epoch,
              shuffle=True,
              verbose = 1)
else:
    print('Using real-time data augmentation.')
    datagen = new_datagen()
   
    # compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied)
    datagen.fit(all_label)

    # fit the model on the batches generated by datagen.flow()
    model.fit_generator(datagen.flow(all_label, label,
                        batch_size=batch_size),
                        samples_per_epoch=all_label.shape[0],
                        nb_epoch=nb_epoch,
			verbose = 0 )
if SemiSupervised:
	#while(True):
	for i in range(15):
		result = model.predict(unlabel, batch_size = batch_size, verbose = 2)
		toRemove = []
		for i in range(result.shape[0]):
			if result[i].max() > 0.999:
				argmax = result[i].argmax()
				result[i] = np.zeros((1,10))
				result[i][argmax] = 1
				toRemove.append(i);

		all_label = np.concatenate((all_label, unlabel[toRemove]))
		label = np.concatenate((label, result[toRemove]))
		unlabel = np.delete(unlabel, toRemove, 0)
		print all_label.shape
		if len(toRemove) == 0  or label.shape[0] >= 20000:
			break;

		model = new_model(all_label.shape[1:])
		if data_augmentation:
			datagen = new_datagen()
			datagen.fit(all_label)
			model.fit_generator(datagen.flow(all_label, label,
					batch_size=batch_size),
					samples_per_epoch=all_label.shape[0],
					nb_epoch=nb_epoch,
					verbose = 0)
		else :
			model.fit(all_label, label,
				batch_size=batch_size,
				nb_epoch=nb_epoch,
				shuffle=True,
				verbose = 0)


model.save(modelName)

#result = model.predict_classes(test,batch_size=batch_size,verbose=2)

#out = open('result.csv','w+')

#out.write("ID,class\n")

#for i in range(10000):
#	out.write(str(i) + ',' + str(result[i]) + '\n')

